{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM41bpBtaWHXiXUwj3goFAo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Import dependencies"],"metadata":{"id":"ajgS5RnAcvf5"}},{"cell_type":"code","source":["!pip install pdfminer.six"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZuK0r0NrjI5u","executionInfo":{"status":"ok","timestamp":1705706599199,"user_tz":-60,"elapsed":10964,"user":{"displayName":"Fida Fida","userId":"16516579540060451036"}},"outputId":"1512290a-33a6-43d6-cd42-32007d8e768a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pdfminer.six\n","  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.7)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n","Installing collected packages: pdfminer.six\n","Successfully installed pdfminer.six-20231228\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","from pdfminer.high_level import extract_text\n","import re\n","import json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ky84ZNJ-vXpO","executionInfo":{"status":"ok","timestamp":1705706657312,"user_tz":-60,"elapsed":58128,"user":{"displayName":"Fida Fida","userId":"16516579540060451036"}},"outputId":"ac6df2c1-9110-46b7-a666-44089af18b21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Key functions"],"metadata":{"id":"tRdz3DuoiWXl"}},{"cell_type":"markdown","source":["A big portion of our data comes in a pdf format. While packages like pdfminer do a great job of extracting the text data, some pdf artifacts inevitably remain."],"metadata":{"id":"Z_cVxgsoc2Bd"}},{"cell_type":"code","source":["def extract_text_from_pdf(file_path):\n","    \"\"\"\n","    Extracts text from a PDF file.\n","\n","    Args:\n","    file_path (str): The file path of the PDF from which to extract text.\n","\n","    Returns:\n","    str: The extracted text.\n","    \"\"\"\n","    try:\n","        text = extract_text(file_path)\n","        return text\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None"],"metadata":{"id":"GhwrQk2gjQe0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We start by removing tab characters (\\t), form-feed characters (x0c, /f), and we further merge word divisions that occur due to newlines (- \\n, - ).\n","\n","Further, while we currently only consider textual data (i.e., we remove any figures, tables or boxes) some sentences that refer to non-textual data are nevertheless useful, as they sometimes present a short-summary of the graphical representation they refer to. In any case, direct references to figures or tables are obviously not something we need our chatbot to learn - we therefore substitute any mention to [Figures, Tables, Boxes] with \"the data\"."],"metadata":{"id":"_TPZJcp4fXsN"}},{"cell_type":"code","source":["def clean_text(text):\n","    # Remove tab characters, word divisions (that occur because of newlines), and some special characters\n","    text = text.replace(\"\\t\", \" \")\n","    text = text.replace(\" \\x0c\", \"\")\n","    text = text.replace(\"x0c\", \"\")\n","    text = text.replace(\"\\f\", \"\")\n","    text = text.replace(\"- \\n\",\"\")\n","    text = text.replace(\"- \",\"\")\n","\n","    # Replace 'Figure' references\n","    # Inside brackets: Replace with \"as shown by the data\"\n","    text = re.sub(r'\\((Figures? \\d+(\\.\\d+)*)\\)', \"as shown by the data\", text)\n","    # Outside brackets: Replace with \"the data\"\n","    text = re.sub(r'Figures? \\d+(\\.\\d+)*', \"the data\", text)\n","\n","    # Replace 'Box' references\n","    # Inside brackets: Replace with \"as shown by the data\"\n","    text = re.sub(r'\\((Box \\d+(\\.\\d+)*)\\)', \"as shown by the data\", text)\n","    # Outside brackets: Replace with \"the data\"\n","    text = re.sub(r'Box \\d+(\\.\\d+)*', \"the data\", text)\n","\n","    # Replace 'Table' references\n","    # Inside brackets: Replace with \"as shown by the data\"\n","    text = re.sub(r'\\((Tables? \\d+(\\.\\d+)*)\\)', \"as shown by the data\", text)\n","    # Outside brackets: Replace with \"the data\"\n","    text = re.sub(r'Tables? \\d+(\\.\\d+)*', \"the data\", text)\n","\n","    return text\n","\n","def split_into_paragraphs(text):\n","    # Split the text into paragraphs at every occurrence of two newline characters\n","    paragraphs = text.split('\\n\\n')\n","\n","    # Optional: Trim whitespace from each paragraph\n","    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n","    cleaned_paragraphs = [paragraph.replace('\\n', ' ').strip() for paragraph in paragraphs if paragraph.strip()]\n","    cleaned_paragraphs = [paragraph.replace('  ', ' ').strip() for paragraph in cleaned_paragraphs if paragraph.strip()]\n","    # Sometimes, paragraphs get broken up by a new page - we identify such cases by looking at paragraphs that\n","    # do not end in classical end tokens, such as [. ! ?], and append to them the subsequent paragraph.\n","    merged_paragraphs = []\n","    paragraph_to_merge = \"\"\n","    for paragraph in cleaned_paragraphs:\n","        # Check if paragraph ends with ., ?, or !\n","        if paragraph and paragraph[-1] in {'.', '?', '!'}:\n","            # If there's a paragraph to merge, merge it first\n","            if paragraph_to_merge:\n","                merged_paragraphs.append(paragraph_to_merge + \" \" + paragraph)\n","                paragraph_to_merge = \"\"\n","            else:\n","                merged_paragraphs.append(paragraph)\n","        else:\n","            # Append this paragraph to the paragraph_to_merge\n","            if paragraph_to_merge:\n","                paragraph_to_merge += \" \" + paragraph\n","            else:\n","                paragraph_to_merge = paragraph\n","\n","    # Add the last paragraph if it hasn't been added yet\n","    if paragraph_to_merge:\n","        merged_paragraphs.append(paragraph_to_merge)\n","\n","    # Calculate lengths of final paragraphs\n","    paragraph_lengths_final = [len(paragraph.split()) for paragraph in merged_paragraphs]\n","    return merged_paragraphs, paragraph_lengths_final"],"metadata":{"id":"S_R_hEuJfSIM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Example usage\n"],"metadata":{"id":"3z0NLNzKdbHx"}},{"cell_type":"code","source":["file_path = 'example.pdf'\n","extracted_text = extract_text_from_pdf(file_path)\n","cleaned_text = clean_text(extracted_text)\n","paragraphs, paragraph_lengths = split_into_paragraphs(cleaned_text)"],"metadata":{"id":"Yqt9GaPzv6Aa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A lot of our data sources have a specific target group or region in mind - we would like to preserve this information when generating summaries and questions, and therefore append region or target-group specific tags to the start of each paragraph."],"metadata":{"id":"Shvl6chzhnBw"}},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Climate Change AIctivist/data/GEO_Youth_paragraphs.txt\", 'w') as file:\n","    for string in paragraphs:\n","        tagged_paragraph = \"[TARGET_GROUP_TAG] \" + string\n","        file.write(tagged_paragraph + '\\n')"],"metadata":{"id":"MshhSC9wv6Ab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Concatenate\n"],"metadata":{"id":"ZVMYfJ4Z5Z4H"}},{"cell_type":"code","source":["import os\n","\n","directory = '/content/drive/MyDrive/Climate Change AIctivist/data'  # Replace with your directory path\n","all_text = \"\"\n","\n","# Read each .txt file and concatenate the content\n","for filename in os.listdir(directory):\n","    if filename.endswith('.txt'):\n","        with open(os.path.join(directory, filename), 'r') as file:\n","            all_text += file.read() + \"\\n\"\n","\n","# Save the concatenated content into a new file\n","with open(os.path.join(\"/content/drive/MyDrive/Climate Change AIctivist/all_data\", 'all_paragraphs.txt'), 'w') as file:\n","    file.write(all_text)"],"metadata":{"id":"vxhognFb5bhI"},"execution_count":null,"outputs":[]}]}